{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "---\n",
    "sidebar_label: CloudflareVectorize\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CloudflareVectorizeVectorStore\n",
    "\n",
    "This notebook covers how to get started with the CloudflareVectorize vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "This Python package is a wrapper around Cloudflare's REST API.  To interact with the API, you need to provide an API token with the appropriate privileges.\n",
    "\n",
    "You can create and manage API tokens here:\n",
    "\n",
    "https://dash.cloudflare.com/YOUR-ACCT-NUMBER/api-tokens\n",
    "\n",
    "### Credentials\n",
    "\n",
    "CloudflareVectorize depends on WorkersAI (if you want to use it for Embeddings), and D1 (if you are using it to store and retrieve raw values).\n",
    "\n",
    "While you can create a single `api_token` with Edit privileges to all needed resources (WorkersAI, Vectorize & D1), you may want to follow the principle of \"least privilege access\" and create separate API tokens for each service\n",
    "\n",
    "**Note:** These service-specific tokens (if provided) will take preference over a global token.  You could provide these instead of a global token.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load_dotenv(\".env\")\n",
    "load_dotenv(\"/Users/collierking/Desktop/github/langchain-cloudflare/docs/.env\")\n",
    "\n",
    "cf_acct_id = os.getenv(\"CF_ACCOUNT_ID\")\n",
    "\n",
    "# single token with WorkersAI, Vectorize & D1\n",
    "api_token = os.getenv(\"CF_VECTORIZE_API_TOKEN\")\n",
    "\n",
    "# OR, separate tokens with access to each service\n",
    "cf_vectorize_token = os.getenv(\"CF_VECTORIZE_API_TOKEN\")\n",
    "cf_d1_token = os.getenv(\"CF_D1_API_TOKEN\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:10:06.883443Z",
     "start_time": "2025-05-10T20:10:06.873699Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import json\n",
    "import uuid\n",
    "import warnings\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_cloudflare.embeddings import (\n",
    "    CloudflareWorkersAIEmbeddings,\n",
    ")\n",
    "from langchain_cloudflare.vectorstores import (\n",
    "    CloudflareVectorize,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:10:08.553197Z",
     "start_time": "2025-05-10T20:10:08.307443Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:10:09.316059Z",
     "start_time": "2025-05-10T20:10:09.313053Z"
    }
   },
   "source": [
    "# name your vectorize index\n",
    "vectorize_index_name = f\"test-langchain-{uuid.uuid4().hex}\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embeddings\n",
    "\n",
    "For storage of embeddings, semantic search and retrieval, you must embed your raw values as embeddings.  Specify an embedding model, one available on WorkersAI\n",
    "\n",
    "[https://developers.cloudflare.com/workers-ai/models/](https://developers.cloudflare.com/workers-ai/models/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:10:10.894410Z",
     "start_time": "2025-05-10T20:10:10.891940Z"
    }
   },
   "source": [
    "MODEL_WORKERSAI = \"@cf/baai/bge-large-en-v1.5\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:10:12.585543Z",
     "start_time": "2025-05-10T20:10:12.582152Z"
    }
   },
   "source": [
    "cf_ai_token = os.getenv(\n",
    "    \"CF_AI_API_TOKEN\"\n",
    ")  # needed if you want to use workersAI for embeddings\n",
    "\n",
    "embedder = CloudflareWorkersAIEmbeddings(\n",
    "    account_id=cf_acct_id,\n",
    "    api_token=cf_ai_token,\n",
    "    model_name=MODEL_WORKERSAI\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Raw Values with D1\n",
    "\n",
    "Vectorize only stores embeddings, metadata and namespaces. If you want to store and retrieve raw values, you must leverage Cloudflare's SQL Database D1.\n",
    "\n",
    "You can create a database here and retrieve its id:\n",
    "\n",
    "[https://dash.cloudflare.com/YOUR-ACCT-NUMBER/workers/d1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:10:14.014885Z",
     "start_time": "2025-05-10T20:10:14.012718Z"
    }
   },
   "source": [
    "# provide the id of your D1 Database\n",
    "d1_database_id = os.getenv(\"CF_D1_DATABASE_ID\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### CloudflareVectorize Class\n",
    "\n",
    "Now we can create the CloudflareVectorize instance.  Here we passed:\n",
    "\n",
    "* The `embedding` instance from earlier\n",
    "* The account ID\n",
    "* A global API token for all services (WorkersAI, Vectorize, D1)\n",
    "* Individual API tokens for each service"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:10:50.385080Z",
     "start_time": "2025-05-10T20:10:50.382196Z"
    }
   },
   "source": [
    "cfVect = CloudflareVectorize(\n",
    "    embedding=embedder,\n",
    "    account_id=cf_acct_id,\n",
    "    d1_api_token=cf_d1_token,  # (Optional if using global token)\n",
    "    vectorize_api_token=cf_vectorize_token,  # (Optional if using global token)\n",
    "    d1_database_id=d1_database_id,  # (Optional if not using D1)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Before we get started, let's delete any `test-langchain*` indexes we have for this walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:10:20.832970Z",
     "start_time": "2025-05-10T20:10:20.504325Z"
    }
   },
   "source": [
    "# depending on your notebook environment you might need to include:\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "arr_indexes = cfVect.list_indexes()\n",
    "arr_indexes = [x for x in arr_indexes if \"test-langchain\" in x.get(\"name\")]\n",
    "arr_async_requests = [\n",
    "    cfVect.adelete_index(index_name=x.get(\"name\")) for x in arr_indexes\n",
    "]\n",
    "await asyncio.gather(*arr_async_requests);"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Gotchyas\n",
    "\n",
    "A few \"gotchyas\" are shown below for various missing token/parameter combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "D1 Database ID provided but no \"global\" `api_token` and no `d1_api_token`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:10:27.985682Z",
     "start_time": "2025-05-10T20:10:27.982954Z"
    }
   },
   "source": [
    "try:\n",
    "    cfVect = CloudflareVectorize(\n",
    "        embedding=embedder,\n",
    "        account_id=cf_acct_id,\n",
    "        # api_token=api_token, # (Optional if using service-specific token)\n",
    "        ai_api_token=cf_ai_token,  # (Optional if using global token)\n",
    "        # d1_api_token=cf_d1_token,  # (Optional if using global token)\n",
    "        vectorize_api_token=cf_vectorize_token,  # (Optional if using global token)\n",
    "        d1_database_id=d1_database_id,  # (Optional if not using D1)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "No \"global\" `api_token` provided and either missing `ai_api_token` or `vectorize_api_token`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "try:\n",
    "    cfVect = CloudflareVectorize(\n",
    "        embedding=embedder,\n",
    "        account_id=cf_acct_id,\n",
    "        # api_token=api_token, # (Optional if using service-specific token)\n",
    "        # ai_api_token=cf_ai_token,  # (Optional if using global token)\n",
    "        d1_api_token=cf_d1_token,  # (Optional if using global token)\n",
    "        vectorize_api_token=cf_vectorize_token,  # (Optional if using global token)\n",
    "        d1_database_id=d1_database_id,  # (Optional if not using D1)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Manage Vector Store\n",
    "\n",
    "### Creating an Index\n",
    "\n",
    "Let's start off this example by creating and index (and first deleting if it exists).  If the index doesn't exist we will get a an error from Cloudflare telling us so."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:10:55.339468Z",
     "start_time": "2025-05-10T20:10:55.336527Z"
    }
   },
   "source": [
    "%%capture\n",
    "\n",
    "try:\n",
    "    cfVect.delete_index(index_name=vectorize_index_name, wait=True)\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:11:10.360383Z",
     "start_time": "2025-05-10T20:11:01.565940Z"
    }
   },
   "source": [
    "r = cfVect.create_index(\n",
    "    index_name=vectorize_index_name,\n",
    "    description=\"A Test Vectorize Index\",\n",
    "    wait=True\n",
    ")\n",
    "print(r)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_on': '2025-05-10T20:11:02.023004Z', 'modified_on': '2025-05-10T20:11:02.023004Z', 'name': 'test-langchain-0ffbaa69fc0a46aca5a528c3411b0dcf', 'description': 'A Test Vectorize Index', 'config': {'dimensions': 1024, 'metric': 'cosine'}}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Listing Indexes\n",
    "\n",
    "Now, we can list our indexes on our account"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:11:10.572644Z",
     "start_time": "2025-05-10T20:11:10.377812Z"
    }
   },
   "source": [
    "indexes = cfVect.list_indexes()\n",
    "indexes = [x for x in indexes if \"test-langchain\" in x.get(\"name\")]\n",
    "print(indexes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'created_on': '2025-05-10T20:11:02.023004Z', 'modified_on': '2025-05-10T20:11:02.023004Z', 'name': 'test-langchain-0ffbaa69fc0a46aca5a528c3411b0dcf', 'description': 'A Test Vectorize Index', 'config': {'dimensions': 1024, 'metric': 'cosine'}}]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Get Index Info\n",
    "We can also get certain indexes and retrieve more granular information about an index.\n",
    "\n",
    "This call returns a `processedUpToMutation` which can be used to track the status of operations such as creating indexes, adding or deleting records."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:11:11.330998Z",
     "start_time": "2025-05-10T20:11:10.576730Z"
    }
   },
   "source": [
    "r = cfVect.get_index_info(index_name=vectorize_index_name)\n",
    "print(r)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimensions': 1024, 'vectorCount': 0}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Adding Metadata Indexes\n",
    "\n",
    "It is common to assist retrieval by supplying metadata filters in quereies.  In Vectorize, this is accomplished by first creating a \"metadata index\" on your Vectorize Index.  We will do so for our example by creating one on the `section` field in our documents.\n",
    "\n",
    "**Reference:** [https://developers.cloudflare.com/vectorize/reference/metadata-filtering/](https://developers.cloudflare.com/vectorize/reference/metadata-filtering/)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:11:30.562066Z",
     "start_time": "2025-05-10T20:11:11.336485Z"
    }
   },
   "source": [
    "r = cfVect.create_metadata_index(\n",
    "    property_name=\"section\",\n",
    "    index_type=\"string\",\n",
    "    index_name=vectorize_index_name,\n",
    "    wait=True,\n",
    ")\n",
    "print(r)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mutationId': '9a337db0-f813-4ddf-9570-36a695c13441'}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Metadata Indexes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:11:30.822686Z",
     "start_time": "2025-05-10T20:11:30.570713Z"
    }
   },
   "source": [
    "r = cfVect.list_metadata_indexes(index_name=vectorize_index_name)\n",
    "print(r)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'propertyName': 'section', 'indexType': 'String'}]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Adding Documents\n",
    "For this example, we will use LangChain's Wikipedia loader to pull an article about Cloudflare.  We will store this in Vectorize and query its contents later."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:11:31.947906Z",
     "start_time": "2025-05-10T20:11:30.834137Z"
    }
   },
   "source": [
    "docs = WikipediaLoader(query=\"Cloudflare\", load_max_docs=2).load()"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will then create some simple chunks with metadata based on the chunk sections."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:11:31.966108Z",
     "start_time": "2025-05-10T20:11:31.959795Z"
    }
   },
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([docs[0].page_content])\n",
    "\n",
    "running_section = \"\"\n",
    "for idx, text in enumerate(texts):\n",
    "    if text.page_content.startswith(\"=\"):\n",
    "        running_section = text.page_content\n",
    "        running_section = running_section.replace(\"=\", \"\").strip()\n",
    "    else:\n",
    "        if running_section == \"\":\n",
    "            text.metadata = {\"section\": \"Introduction\"}\n",
    "        else:\n",
    "            text.metadata = {\"section\": running_section}"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:11:31.976245Z",
     "start_time": "2025-05-10T20:11:31.973651Z"
    }
   },
   "source": [
    "print(len(texts))\n",
    "print(texts[0], \"\\n\\n\", texts[-1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "page_content='Cloudflare, Inc., is an American company that provides content delivery network services,' metadata={'section': 'Introduction'} \n",
      "\n",
      " page_content='attacks, Cloudflare ended up being attacked as well; Google and other companies eventually' metadata={'section': 'DDoS mitigation'}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we will add documents to our Vectorize Index.\n",
    "\n",
    "**Note:**\n",
    "Adding embeddings to Vectorize happens `asyncronously`, meaning there will be a small delay between adding the embeddings and being able to query them.  By default `add_documents` has a `wait=True` parameter which waits for this operation to complete before returning a response.  If you do not want the program to wait for embeddings availability, you can set this to `wait=False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:11:55.433527Z",
     "start_time": "2025-05-10T20:11:31.981911Z"
    }
   },
   "source": [
    "r = cfVect.add_documents(index_name=vectorize_index_name, documents=texts, wait=True)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:11:55.447801Z",
     "start_time": "2025-05-10T20:11:55.444061Z"
    }
   },
   "source": [
    "print(json.dumps(r)[:300])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"6d464cfa-5921-4fff-a620-19cd243bd72b\", \"4e8731b7-cb6b-4331-a9b5-ea8faec71999\", \"132fb3d4-f95d-4210-ad85-20d76f4a3523\", \"e687e72a-dbc0-4ade-a530-48025eee3ac0\", \"54dd6dcc-7a82-47c4-b230-ab6743a26b95\", \"15d274df-3884-4d79-8e0b-11ed35afadf8\", \"e2d35e91-12cf-473a-a958-3a4013b385b0\", \"0e01211f-10cc-4138\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Query vector store\n",
    "\n",
    "We will do some searches on our embeddings.  We can specify our search `query` and the top number of results we want with `k`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:00.094863Z",
     "start_time": "2025-05-10T20:11:55.455288Z"
    }
   },
   "source": [
    "query_documents = cfVect.similarity_search(\n",
    "    index_name=vectorize_index_name, query=\"Workers AI\", k=100, return_metadata=\"none\"\n",
    ")\n",
    "\n",
    "print(f\"{len(query_documents)} results:\\n{query_documents[:3]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 results:\n",
      "[Document(id='d7dd7ff6-0c42-4e35-84a3-79d637d8da73', metadata={}, page_content=\"In 2023, Cloudflare launched Workers AI, a framework allowing for use of Nvidia GPU's within\"), Document(id='63767874-6ce3-45dd-bde0-1b546364bf6d', metadata={}, page_content='based on queries by leveraging Workers AI.Cloudflare announced plans in September 2024 to launch a'), Document(id='52d3295b-5eaa-44a7-9831-bb893b41cc51', metadata={}, page_content='=== Artificial intelligence ===')]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Output\n",
    "\n",
    "If you want to return metadata you can pass `return_metadata=\"all\" | 'indexed'`.  The default is `all`.\n",
    "\n",
    "If you want to return the embeddings values, you can pass `return_values=True`.  The default is `False`.\n",
    "Embeddings will be returned in the `metadata` field under the special `_values` field.\n",
    "\n",
    "**Note:** `return_metadata=\"none\"` and `return_values=True` will return only ther `_values` field in `metadata`.\n",
    "\n",
    "**Note:**\n",
    "If you return metadata or values, the results will be limited to the top 20.\n",
    "\n",
    "[https://developers.cloudflare.com/vectorize/platform/limits/](https://developers.cloudflare.com/vectorize/platform/limits/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:02.228370Z",
     "start_time": "2025-05-10T20:12:00.108229Z"
    }
   },
   "source": [
    "query_documents = cfVect.similarity_search(\n",
    "    index_name=vectorize_index_name,\n",
    "    query=\"Workers AI\",\n",
    "    return_values=True,\n",
    "    return_metadata=\"all\",\n",
    "    k=100,\n",
    ")\n",
    "print(f\"{len(query_documents)} results:\\n{str(query_documents[0])[:500]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 results:\n",
      "page_content='In 2023, Cloudflare launched Workers AI, a framework allowing for use of Nvidia GPU's within' metadata={'section': 'Artificial intelligence', '_values': [0.014350891, 0.0053482056, -0.022354126, 0.002948761, 0.010406494, -0.016067505, -0.002029419, -0.023513794, 0.020141602, 0.023742676, 0.01361084, 0.003019333, 0.02748108, -0.023162842, 0.008979797, -0.029373169, -0.03643799, -0.03842163, -0.004463196, 0.021255493, 0.02192688, -0.005947113, -0.060272217, -0.055389404, -0.031188965\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you'd like the similarity `scores` to be returned, you can use `similarity_search_with_score`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:03.770364Z",
     "start_time": "2025-05-10T20:12:02.250181Z"
    }
   },
   "source": [
    "query_documents = cfVect.similarity_search_with_score(\n",
    "    index_name=vectorize_index_name,\n",
    "    query=\"Workers AI\",\n",
    "    k=100,\n",
    "    return_metadata=\"all\",\n",
    ")\n",
    "print(f\"{len(query_documents)} results:\\n{str(query_documents[0])[:500]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 results:\n",
      "(Document(id='d7dd7ff6-0c42-4e35-84a3-79d637d8da73', metadata={'section': 'Artificial intelligence'}, page_content=\"In 2023, Cloudflare launched Workers AI, a framework allowing for use of Nvidia GPU's within\"), 0.7851709)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Including D1 for \"Raw Values\"\n",
    "All of the `add` and `search` methods on CloudflareVectorize support a `include_d1` parameter (default=True).\n",
    "\n",
    "This is to configure whether you want to store/retrieve raw values.\n",
    "\n",
    "If you do not want to use D1 for this, you can set this to `include=False`.  This will return documents with an empty `page_content` field.\n",
    "\n",
    "**Note:** Your D1 table name MUST MATCH your vectorize index name!  If you run 'create_index' and include_d1=True or  cfVect(d1_database=...,) this D1 table will be created along with your Vectorize Index."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:04.769677Z",
     "start_time": "2025-05-10T20:12:03.777454Z"
    }
   },
   "source": [
    "query_documents = cfVect.similarity_search_with_score(\n",
    "    index_name=vectorize_index_name,\n",
    "    query=\"california\",\n",
    "    k=100,\n",
    "    return_metadata=\"all\",\n",
    "    include_d1=False,\n",
    ")\n",
    "print(f\"{len(query_documents)} results:\\n{str(query_documents[0])[:500]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 results:\n",
      "(Document(id='e687e72a-dbc0-4ade-a530-48025eee3ac0', metadata={'section': 'Introduction'}, page_content=''), 0.60426825)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query by turning into retriever\n",
    "\n",
    "You can also transform the vector store into a retriever for easier usage in your chains. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "retriever = cfVect.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1, \"index_name\": vectorize_index_name},\n",
    ")\n",
    "r = retriever.get_relevant_documents(\"california\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:06.280652Z",
     "start_time": "2025-05-10T20:12:04.781676Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Searching with Metadata Filtering\n",
    "\n",
    "As mentioned before, Vectorize supports filtered search via filtered on indexes metadata fields.  Here is an example where we search for `Introduction` values within the indexed `section` metadata field.\n",
    "\n",
    "More info on searching on Metadata fields is here: [https://developers.cloudflare.com/vectorize/reference/metadata-filtering/](https://developers.cloudflare.com/vectorize/reference/metadata-filtering/)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:08.637885Z",
     "start_time": "2025-05-10T20:12:06.292593Z"
    }
   },
   "source": [
    "query_documents = cfVect.similarity_search_with_score(\n",
    "    index_name=vectorize_index_name,\n",
    "    query=\"California\",\n",
    "    k=100,\n",
    "    md_filter={\"section\": \"Introduction\"},\n",
    "    return_metadata=\"all\",\n",
    ")\n",
    "print(f\"{len(query_documents)} results:\\n - {str(query_documents[:3])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 results:\n",
      " - [(Document(id='e687e72a-dbc0-4ade-a530-48025eee3ac0', metadata={'section': 'Introduction'}, page_content=\"and other services. Cloudflare's headquarters are in San Francisco, California. According to\"), 0.60426825), (Document(id='4e8731b7-cb6b-4331-a9b5-ea8faec71999', metadata={'section': 'Introduction'}, page_content='network services, cybersecurity, DDoS mitigation, wide area network services, reverse proxies,'), 0.52082914), (Document(id='6d464cfa-5921-4fff-a620-19cd243bd72b', metadata={'section': 'Introduction'}, page_content='Cloudflare, Inc., is an American company that provides content delivery network services,'), 0.50490546)]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do more sophisticated filtering as well\n",
    "\n",
    "https://developers.cloudflare.com/vectorize/reference/metadata-filtering/#valid-filter-examples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:10.800736Z",
     "start_time": "2025-05-10T20:12:08.650256Z"
    }
   },
   "source": [
    "query_documents = cfVect.similarity_search_with_score(\n",
    "    index_name=vectorize_index_name,\n",
    "    query=\"California\",\n",
    "    k=100,\n",
    "    md_filter={\"section\": {\"$ne\": \"Introduction\"}},\n",
    "    return_metadata=\"all\",\n",
    ")\n",
    "print(f\"{len(query_documents)} results:\\n - {str(query_documents[:3])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 results:\n",
      " - [(Document(id='8562367f-94bd-4c83-80f4-084471784ea7', metadata={}, page_content='== Products =='), 0.56540567), (Document(id='a10e3744-c947-4372-9495-16da1764986f', metadata={'section': 'History'}, page_content='Since at least 2017, Cloudflare has been using a wall of lava lamps in their San Francisco'), 0.5604333), (Document(id='ae8e7d76-4b45-4d98-a6a3-77179b457f45', metadata={'section': 'History'}, page_content='their San Francisco headquarters as a source of randomness for encryption keys, alongside double'), 0.55573463)]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:12.129500Z",
     "start_time": "2025-05-10T20:12:10.813837Z"
    }
   },
   "source": [
    "query_documents = cfVect.similarity_search_with_score(\n",
    "    index_name=vectorize_index_name,\n",
    "    query=\"DNS\",\n",
    "    k=100,\n",
    "    md_filter={\"section\": {\"$in\": [\"Products\", \"History\"]}},\n",
    "    return_metadata=\"all\",\n",
    ")\n",
    "print(f\"{len(query_documents)} results:\\n - {str(query_documents)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 results:\n",
      " - [(Document(id='98ec32a0-6538-4175-83df-61952ab5fca9', metadata={'section': 'Products'}, page_content='protocols such as DNS over HTTPS, SMTP, and HTTP/2 with support for HTTP/2 Server Push. As of 2023,'), 0.7205538), (Document(id='e4bd6d2f-4d36-48ee-a9ca-cddcff746551', metadata={'section': 'Products'}, page_content='utilizing edge computing, reverse proxies for web traffic, data center interconnects, and a content'), 0.58178145), (Document(id='26ff9000-ebee-4822-9371-d9562168b974', metadata={'section': 'Products'}, page_content='and a content distribution network to serve content across its network of servers. It supports'), 0.5797795), (Document(id='98d364e1-41f0-418f-8774-7049bdd6df6c', metadata={'section': 'History'}, page_content='the New York Stock Exchange under the stock ticker NET. It opened for public trading on September'), 0.5678468), (Document(id='7f544ea3-58b8-494e-b61d-bdfc7ffe96fe', metadata={'section': 'Products'}, page_content='Cloudflare provides network and security products for consumers and businesses, utilizing edge'), 0.55722594), (Document(id='28552da4-f742-4fc6-a254-4bc6614e6ed4', metadata={'section': 'History'}, page_content='Cloudflare has acquired web-services and security companies, including StopTheHacker (February'), 0.5558441), (Document(id='6c6d3220-5ebe-455c-8464-6427a1eb54fa', metadata={'section': 'Products'}, page_content='Push. As of 2023, Cloudflare handles an average of 45 million HTTP requests per second.'), 0.55429655), (Document(id='b5e0eea8-a764-44a3-993c-75b9f4a4a6ee', metadata={'section': 'Products'}, page_content='It supports transport layer protocols TCP, UDP, QUIC, and many application layer protocols such as'), 0.54969466), (Document(id='0e01211f-10cc-4138-a912-a6004149d878', metadata={'section': 'History'}, page_content='Cloudflare was founded in July 2009 by Matthew Prince, Lee Holloway, and Michelle Zatlyn. Prince'), 0.54691005), (Document(id='7ed0b07d-6a27-44bd-bfa8-87b87296dbbe', metadata={'section': 'History'}, page_content='2019, Cloudflare submitted its S-1 filing for an initial public offering on the New York Stock'), 0.533554), (Document(id='2e61ed83-4842-49e8-8537-ef757a515687', metadata={'section': 'History'}, page_content='Networks (March 2024), BastionZero (May 2024), and Kivera (October 2024).'), 0.53296596), (Document(id='f97d1adb-9f2b-4082-80db-d8bad35c7c06', metadata={'section': 'Products'}, page_content='Verizon’s October 2024 outage.'), 0.53137076), (Document(id='eaf69be7-6a67-4b14-b495-5d11d3907c2d', metadata={'section': 'Products'}, page_content='Cloudflare also provides analysis and reports on large-scale outages, including Verizon’s October'), 0.53107977), (Document(id='1ef0ae69-1378-4153-93f8-96f623c3ef35', metadata={'section': 'History'}, page_content='a product of Unspam Technologies that served as some inspiration for the basis of Cloudflare. From'), 0.528889), (Document(id='eaae308d-075c-4943-8802-62c62b01a807', metadata={'section': 'History'}, page_content='of Cloudflare. From 2009, the company was venture-capital funded. On August 15, 2019, Cloudflare'), 0.52717584), (Document(id='78bd27c9-d6e6-46c7-89db-ef817f8cbc89', metadata={'section': 'History'}, page_content='(December 2021), Vectrix (February 2022), Area 1 Security (February 2022), Nefeli Networks (March'), 0.52209044), (Document(id='9804c35a-0e1b-4ff7-b1f8-7f15d4d0f0e8', metadata={'section': 'Products'}, page_content='As of 2024, Cloudflare servers are powered by AMD EPYC 9684X processors.'), 0.5169676), (Document(id='9b8f6b21-6155-4a46-9422-9a4306dc6157', metadata={'section': 'History'}, page_content='(February 2014), CryptoSeal (June 2014), Eager Platform Co. (December 2016), Neumob (November'), 0.5132974), (Document(id='ae8e7d76-4b45-4d98-a6a3-77179b457f45', metadata={'section': 'History'}, page_content='their San Francisco headquarters as a source of randomness for encryption keys, alongside double'), 0.50999177), (Document(id='2d95414a-ed22-40c3-a88a-fdb9b8a604b6', metadata={'section': 'History'}, page_content='Neumob (November 2017), S2 Systems (January 2020), Linc (December 2020), Zaraz (December 2021),'), 0.5092492)]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search by Namespace\n",
    "We can also search for vectors by `namespace`.  We just need to add it to the `namespaces` array when adding it to our vector database.\n",
    "\n",
    "https://developers.cloudflare.com/vectorize/reference/metadata-filtering/#namespace-versus-metadata-filtering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:31.999611Z",
     "start_time": "2025-05-10T20:12:12.152312Z"
    }
   },
   "source": [
    "namespace_name = f\"test-namespace-{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "new_documents = [\n",
    "    Document(\n",
    "        page_content=\"This is a new namespace specific document!\",\n",
    "        metadata={\"section\": \"Namespace Test1\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"This is another namespace specific document!\",\n",
    "        metadata={\"section\": \"Namespace Test2\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "r = cfVect.add_documents(\n",
    "    index_name=vectorize_index_name,\n",
    "    documents=new_documents,\n",
    "    namespaces=[namespace_name] * len(new_documents),\n",
    "    wait=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:35.341883Z",
     "start_time": "2025-05-10T20:12:32.007525Z"
    }
   },
   "source": [
    "query_documents = cfVect.similarity_search(\n",
    "    index_name=vectorize_index_name,\n",
    "    query=\"California\",\n",
    "    namespace=namespace_name,\n",
    ")\n",
    "\n",
    "print(f\"{len(query_documents)} results:\\n - {str(query_documents)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 results:\n",
      " - [Document(id='560afc73-1ae3-414e-be93-243147aaf615', metadata={'section': 'Namespace Test2', '_namespace': 'test-namespace-7e1992d1'}, page_content='This is another namespace specific document!'), Document(id='9a146549-19a8-4f2e-b853-e1dd5fde9dd0', metadata={'section': 'Namespace Test1', '_namespace': 'test-namespace-7e1992d1'}, page_content='This is a new namespace specific document!')]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Search by IDs\n",
    "We can also retrieve specific records for specific IDs.  To do so, we need to set the vectorize index name on the `index_name` Vectorize state param.\n",
    "\n",
    "This will return both `_namespace` and `_values` as well as other `metadata`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:35.367054Z",
     "start_time": "2025-05-10T20:12:35.364522Z"
    }
   },
   "source": [
    "sample_ids = [x.id for x in query_documents]"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:35.380384Z",
     "start_time": "2025-05-10T20:12:35.378488Z"
    }
   },
   "source": [
    "cfVect.index_name = vectorize_index_name"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:37.617645Z",
     "start_time": "2025-05-10T20:12:35.386182Z"
    }
   },
   "source": [
    "query_documents = cfVect.get_by_ids(\n",
    "    sample_ids,\n",
    ")\n",
    "print(str(query_documents[:3])[:500])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='560afc73-1ae3-414e-be93-243147aaf615', metadata={'section': 'Namespace Test2', '_namespace': 'test-namespace-7e1992d1', '_values': [-0.0005841255, 0.014480591, 0.040771484, 0.005218506, 0.015579224, 0.0007543564, -0.005138397, -0.022720337, 0.021835327, 0.038970947, 0.017456055, 0.022705078, 0.013450623, -0.015686035, -0.019119263, -0.01512146, -0.017471313, -0.007183075, -0.054382324, -0.01914978, 0.0005302429, 0.018600464, -0.083740234, -0.006462097, 0.0005598068, 0.024230957, -0\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The namespace will be included in the `_namespace` field in `metadata` along with your other metadata (if you requested it in `return_metadata`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You cannot set the `_namespace` or `_values` fields in `metadata` as they are reserved.  They will be stripped out during the insert process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upserts\n",
    "\n",
    "Vectorize supports Upserts which you can perform by setting `upsert=True`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:37.639989Z",
     "start_time": "2025-05-10T20:12:37.637092Z"
    }
   },
   "source": [
    "query_documents[0].page_content = \"Updated: \" + query_documents[0].page_content\n",
    "print(query_documents[0].page_content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: This is another namespace specific document!\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:12:37.651794Z",
     "start_time": "2025-05-10T20:12:37.649775Z"
    }
   },
   "source": [
    "new_document_id = \"12345678910\"\n",
    "new_document = Document(\n",
    "    id=new_document_id,\n",
    "    page_content=\"This is a new document!\",\n",
    "    metadata={\"section\": \"Introduction\"},\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:13:06.472045Z",
     "start_time": "2025-05-10T20:12:37.665826Z"
    }
   },
   "source": [
    "r = cfVect.add_documents(\n",
    "    index_name=vectorize_index_name,\n",
    "    documents=[new_document, query_documents[0]],\n",
    "    upsert=True,\n",
    "    wait=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:13:09.191474Z",
     "start_time": "2025-05-10T20:13:06.477010Z"
    }
   },
   "source": [
    "query_documents_updated = cfVect.get_by_ids([new_document_id, query_documents[0].id])"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:13:09.210644Z",
     "start_time": "2025-05-10T20:13:09.206805Z"
    }
   },
   "source": [
    "print(str(query_documents_updated[0])[:500])\n",
    "print(query_documents_updated[0].page_content)\n",
    "print(query_documents_updated[1].page_content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='This is a new document!' metadata={'section': 'Introduction', '_namespace': None, '_values': [-0.007522583, 0.0023021698, 0.009963989, 0.031051636, -0.021316528, 0.0048103333, 0.026046753, 0.01348114, 0.026306152, 0.040374756, 0.03225708, 0.007423401, 0.031021118, -0.007347107, -0.034179688, 0.002111435, -0.027191162, -0.020950317, -0.021636963, -0.0030593872, -0.04977417, 0.018859863, -0.08062744, -0.027679443, 0.012512207, 0.0053634644, 0.008079529, -0.010528564, 0.07312012, 0.02\n",
      "This is a new document!\n",
      "Updated: This is another namespace specific document!\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Deleting Records\n",
    "We can delete records by their ids as well\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:13:26.673457Z",
     "start_time": "2025-05-10T20:13:09.222543Z"
    }
   },
   "source": [
    "r = cfVect.delete(index_name=vectorize_index_name, ids=sample_ids, wait=True)\n",
    "print(r)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And to confirm deletion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:13:28.098876Z",
     "start_time": "2025-05-10T20:13:26.689422Z"
    }
   },
   "source": [
    "query_documents = cfVect.get_by_ids(sample_ids)\n",
    "assert len(query_documents) == 0"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating from Documents\n",
    "LangChain stipulates that all vectorstores must have a `from_documents` method to instantiate a new Vectorstore from documents.  This is a more streamlined method than the individual `create, add` steps shown above.\n",
    "\n",
    "You can do that as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:13:28.116218Z",
     "start_time": "2025-05-10T20:13:28.113523Z"
    }
   },
   "source": [
    "vectorize_index_name = \"test-langchain-from-docs\""
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:14:11.551409Z",
     "start_time": "2025-05-10T20:13:28.126090Z"
    }
   },
   "source": [
    "cfVect = CloudflareVectorize.from_documents(\n",
    "    account_id=cf_acct_id,\n",
    "    index_name=vectorize_index_name,\n",
    "    documents=texts,\n",
    "    embedding=embedder,\n",
    "    d1_database_id=d1_database_id,\n",
    "    d1_api_token=cf_d1_token,\n",
    "    vectorize_api_token=cf_vectorize_token,\n",
    "    wait=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:14:16.331600Z",
     "start_time": "2025-05-10T20:14:11.558741Z"
    }
   },
   "source": [
    "# query for documents\n",
    "query_documents = cfVect.similarity_search(\n",
    "    index_name=vectorize_index_name,\n",
    "    query=\"Edge Computing\",\n",
    ")\n",
    "\n",
    "print(f\"{len(query_documents)} results:\\n{str(query_documents[0])[:300]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 results:\n",
      "page_content='utilizing edge computing, reverse proxies for web traffic, data center interconnects, and a content' metadata={'section': 'Products'}\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Async Examples\n",
    "This section will show some Async examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Creating Indexes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T20:14:16.347632Z",
     "start_time": "2025-05-10T20:14:16.344496Z"
    }
   },
   "source": [
    "vectorize_index_name1 = f\"test-langchain-{uuid.uuid4().hex}\"\n",
    "vectorize_index_name2 = f\"test-langchain-{uuid.uuid4().hex}\"\n",
    "vectorize_index_name3 = f\"test-langchain-{uuid.uuid4().hex}\""
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T20:14:21.795427Z",
     "start_time": "2025-05-10T20:14:16.356109Z"
    }
   },
   "source": [
    "# depending on your notebook environment you might need to include these:\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "async_requests = [\n",
    "    cfVect.acreate_index(index_name=vectorize_index_name1),\n",
    "    cfVect.acreate_index(index_name=vectorize_index_name2),\n",
    "    cfVect.acreate_index(index_name=vectorize_index_name3),\n",
    "]\n",
    "\n",
    "res = await asyncio.gather(*async_requests);"
   ],
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mReadTimeout\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001B[39m, in \u001B[36mmap_httpcore_exceptions\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    100\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001B[39m, in \u001B[36mAsyncHTTPTransport.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    393\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m394\u001B[39m     resp = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pool.handle_async_request(req)\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001B[39m, in \u001B[36mAsyncConnectionPool.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    255\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._close_connections(closing)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001B[39m, in \u001B[36mAsyncConnectionPool.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    235\u001B[39m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m connection.handle_async_request(\n\u001B[32m    237\u001B[39m         pool_request.request\n\u001B[32m    238\u001B[39m     )\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpcore/_async/connection.py:103\u001B[39m, in \u001B[36mAsyncHTTPConnection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    101\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection.handle_async_request(request)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpcore/_async/http11.py:136\u001B[39m, in \u001B[36mAsyncHTTP11Connection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    135\u001B[39m         \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._response_closed()\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpcore/_async/http11.py:106\u001B[39m, in \u001B[36mAsyncHTTP11Connection.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[32m     98\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mreceive_response_headers\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs\n\u001B[32m     99\u001B[39m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    100\u001B[39m     (\n\u001B[32m    101\u001B[39m         http_version,\n\u001B[32m    102\u001B[39m         status,\n\u001B[32m    103\u001B[39m         reason_phrase,\n\u001B[32m    104\u001B[39m         headers,\n\u001B[32m    105\u001B[39m         trailing_data,\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m     ) = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._receive_response_headers(**kwargs)\n\u001B[32m    107\u001B[39m     trace.return_value = (\n\u001B[32m    108\u001B[39m         http_version,\n\u001B[32m    109\u001B[39m         status,\n\u001B[32m    110\u001B[39m         reason_phrase,\n\u001B[32m    111\u001B[39m         headers,\n\u001B[32m    112\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpcore/_async/http11.py:177\u001B[39m, in \u001B[36mAsyncHTTP11Connection._receive_response_headers\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m     event = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._receive_event(timeout=timeout)\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Response):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpcore/_async/http11.py:217\u001B[39m, in \u001B[36mAsyncHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     data = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._network_stream.read(\n\u001B[32m    218\u001B[39m         \u001B[38;5;28mself\u001B[39m.READ_NUM_BYTES, timeout=timeout\n\u001B[32m    219\u001B[39m     )\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpcore/_backends/anyio.py:32\u001B[39m, in \u001B[36mAnyIOStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m     26\u001B[39m exc_map = {\n\u001B[32m     27\u001B[39m     \u001B[38;5;167;01mTimeoutError\u001B[39;00m: ReadTimeout,\n\u001B[32m     28\u001B[39m     anyio.BrokenResourceError: ReadError,\n\u001B[32m     29\u001B[39m     anyio.ClosedResourceError: ReadError,\n\u001B[32m     30\u001B[39m     anyio.EndOfStream: ReadError,\n\u001B[32m     31\u001B[39m }\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmap_exceptions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexc_map\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43manyio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfail_after\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m     \u001B[38;5;28mself\u001B[39m.gen.throw(typ, value, traceback)\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    160\u001B[39m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[32m    161\u001B[39m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001B[39m, in \u001B[36mmap_exceptions\u001B[39m\u001B[34m(map)\u001B[39m\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exc, from_exc):\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m to_exc(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mReadTimeout\u001B[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mReadTimeout\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[46]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# depending on your notebook environment you might need to include these:\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# import nest_asyncio\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# nest_asyncio.apply()\u001B[39;00m\n\u001B[32m      5\u001B[39m async_requests = [\n\u001B[32m      6\u001B[39m     cfVect.acreate_index(index_name=vectorize_index_name1),\n\u001B[32m      7\u001B[39m     cfVect.acreate_index(index_name=vectorize_index_name2),\n\u001B[32m      8\u001B[39m     cfVect.acreate_index(index_name=vectorize_index_name3),\n\u001B[32m      9\u001B[39m ]\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m res = \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(*async_requests);\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/github/langchain-cloudflare/libs/langchain-cloudflare/langchain_cloudflare/vectorstores.py:2583\u001B[39m, in \u001B[36mCloudflareVectorize.acreate_index\u001B[39m\u001B[34m(self, dimensions, metric, index_name, description, include_d1, wait, **kwargs)\u001B[39m\n\u001B[32m   2580\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mhttpx\u001B[39;00m\n\u001B[32m   2582\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m httpx.AsyncClient() \u001B[38;5;28;01mas\u001B[39;00m client:\n\u001B[32m-> \u001B[39m\u001B[32m2583\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m client.post(\n\u001B[32m   2584\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.base_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/accounts/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.account_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/vectorize/v2/indexes\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2585\u001B[39m         headers=headers,\n\u001B[32m   2586\u001B[39m         json=data,\n\u001B[32m   2587\u001B[39m         **kwargs,\n\u001B[32m   2588\u001B[39m     )\n\u001B[32m   2589\u001B[39m     response.raise_for_status()\n\u001B[32m   2591\u001B[39m     response_data = response.json()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpx/_client.py:1859\u001B[39m, in \u001B[36mAsyncClient.post\u001B[39m\u001B[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[39m\n\u001B[32m   1838\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1839\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1840\u001B[39m     url: URL | \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1852\u001B[39m     extensions: RequestExtensions | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1853\u001B[39m ) -> Response:\n\u001B[32m   1854\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1855\u001B[39m \u001B[33;03m    Send a `POST` request.\u001B[39;00m\n\u001B[32m   1856\u001B[39m \n\u001B[32m   1857\u001B[39m \u001B[33;03m    **Parameters**: See `httpx.request`.\u001B[39;00m\n\u001B[32m   1858\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1859\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.request(\n\u001B[32m   1860\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPOST\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1861\u001B[39m         url,\n\u001B[32m   1862\u001B[39m         content=content,\n\u001B[32m   1863\u001B[39m         data=data,\n\u001B[32m   1864\u001B[39m         files=files,\n\u001B[32m   1865\u001B[39m         json=json,\n\u001B[32m   1866\u001B[39m         params=params,\n\u001B[32m   1867\u001B[39m         headers=headers,\n\u001B[32m   1868\u001B[39m         cookies=cookies,\n\u001B[32m   1869\u001B[39m         auth=auth,\n\u001B[32m   1870\u001B[39m         follow_redirects=follow_redirects,\n\u001B[32m   1871\u001B[39m         timeout=timeout,\n\u001B[32m   1872\u001B[39m         extensions=extensions,\n\u001B[32m   1873\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpx/_client.py:1540\u001B[39m, in \u001B[36mAsyncClient.request\u001B[39m\u001B[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[39m\n\u001B[32m   1525\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m   1527\u001B[39m request = \u001B[38;5;28mself\u001B[39m.build_request(\n\u001B[32m   1528\u001B[39m     method=method,\n\u001B[32m   1529\u001B[39m     url=url,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1538\u001B[39m     extensions=extensions,\n\u001B[32m   1539\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1540\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpx/_client.py:1629\u001B[39m, in \u001B[36mAsyncClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m   1625\u001B[39m \u001B[38;5;28mself\u001B[39m._set_timeout(request)\n\u001B[32m   1627\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m-> \u001B[39m\u001B[32m1629\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_handling_auth(\n\u001B[32m   1630\u001B[39m     request,\n\u001B[32m   1631\u001B[39m     auth=auth,\n\u001B[32m   1632\u001B[39m     follow_redirects=follow_redirects,\n\u001B[32m   1633\u001B[39m     history=[],\n\u001B[32m   1634\u001B[39m )\n\u001B[32m   1635\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1636\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpx/_client.py:1657\u001B[39m, in \u001B[36mAsyncClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m   1654\u001B[39m request = \u001B[38;5;28;01mawait\u001B[39;00m auth_flow.\u001B[34m__anext__\u001B[39m()\n\u001B[32m   1656\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1657\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_handling_redirects(\n\u001B[32m   1658\u001B[39m         request,\n\u001B[32m   1659\u001B[39m         follow_redirects=follow_redirects,\n\u001B[32m   1660\u001B[39m         history=history,\n\u001B[32m   1661\u001B[39m     )\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpx/_client.py:1694\u001B[39m, in \u001B[36mAsyncClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m   1691\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1692\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m hook(request)\n\u001B[32m-> \u001B[39m\u001B[32m1694\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._send_single_request(request)\n\u001B[32m   1695\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1696\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mresponse\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpx/_client.py:1730\u001B[39m, in \u001B[36mAsyncClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1725\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1726\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAttempted to send an sync request with an AsyncClient instance.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1727\u001B[39m     )\n\u001B[32m   1729\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1730\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m transport.handle_async_request(request)\n\u001B[32m   1732\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, AsyncByteStream)\n\u001B[32m   1733\u001B[39m response.request = request\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpx/_transports/default.py:393\u001B[39m, in \u001B[36mAsyncHTTPTransport.handle_async_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mhttpcore\u001B[39;00m\n\u001B[32m    381\u001B[39m req = httpcore.Request(\n\u001B[32m    382\u001B[39m     method=request.method,\n\u001B[32m    383\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    391\u001B[39m     extensions=request.extensions,\n\u001B[32m    392\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmap_httpcore_exceptions\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    394\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresp\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mawait\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_pool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_async_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    156\u001B[39m     value = typ()\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m     \u001B[38;5;28mself\u001B[39m.gen.throw(typ, value, traceback)\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    160\u001B[39m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[32m    161\u001B[39m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n\u001B[32m    163\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m value\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langgraph-multigraph-wv5j0Q5e-py3.11/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001B[39m, in \u001B[36mmap_httpcore_exceptions\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m    117\u001B[39m message = \u001B[38;5;28mstr\u001B[39m(exc)\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m mapped_exc(message) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n",
      "\u001B[31mReadTimeout\u001B[39m: "
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Creating Metadata Indexes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "async_requests = [\n",
    "    cfVect.acreate_metadata_index(\n",
    "        property_name=\"section\",\n",
    "        index_type=\"string\",\n",
    "        index_name=vectorize_index_name1,\n",
    "        wait=True,\n",
    "    ),\n",
    "    cfVect.acreate_metadata_index(\n",
    "        property_name=\"section\",\n",
    "        index_type=\"string\",\n",
    "        index_name=vectorize_index_name2,\n",
    "        wait=True,\n",
    "    ),\n",
    "    cfVect.acreate_metadata_index(\n",
    "        property_name=\"section\",\n",
    "        index_type=\"string\",\n",
    "        index_name=vectorize_index_name3,\n",
    "        wait=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "await asyncio.gather(*async_requests);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Adding Documents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "async_requests = [\n",
    "    cfVect.aadd_documents(index_name=vectorize_index_name1, documents=texts, wait=True),\n",
    "    cfVect.aadd_documents(index_name=vectorize_index_name2, documents=texts, wait=True),\n",
    "    cfVect.aadd_documents(index_name=vectorize_index_name3, documents=texts, wait=True),\n",
    "]\n",
    "\n",
    "await asyncio.gather(*async_requests);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Querying/Search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "async_requests = [\n",
    "    cfVect.asimilarity_search(index_name=vectorize_index_name1, query=\"Workers AI\"),\n",
    "    cfVect.asimilarity_search(index_name=vectorize_index_name2, query=\"Edge Computing\"),\n",
    "    cfVect.asimilarity_search(index_name=vectorize_index_name3, query=\"SASE\"),\n",
    "]\n",
    "\n",
    "async_results = await asyncio.gather(*async_requests);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"{len(async_results[0])} results:\\n{str(async_results[0][0])[:300]}\")\n",
    "print(f\"{len(async_results[1])} results:\\n{str(async_results[1][0])[:300]}\")\n",
    "print(f\"{len(async_results[1])} results:\\n{str(async_results[2][0])[:300]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Returning Metadata/Values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "async_requests = [\n",
    "    cfVect.asimilarity_search(\n",
    "        index_name=vectorize_index_name1,\n",
    "        query=\"California\",\n",
    "        return_values=True,\n",
    "        return_metadata=\"all\",\n",
    "    ),\n",
    "    cfVect.asimilarity_search(\n",
    "        index_name=vectorize_index_name2,\n",
    "        query=\"California\",\n",
    "        return_values=True,\n",
    "        return_metadata=\"all\",\n",
    "    ),\n",
    "    cfVect.asimilarity_search(\n",
    "        index_name=vectorize_index_name3,\n",
    "        query=\"California\",\n",
    "        return_values=True,\n",
    "        return_metadata=\"all\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "async_results = await asyncio.gather(*async_requests);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"{len(async_results[0])} results:\\n{str(async_results[0][0])[:300]}\")\n",
    "print(f\"{len(async_results[1])} results:\\n{str(async_results[1][0])[:300]}\")\n",
    "print(f\"{len(async_results[1])} results:\\n{str(async_results[2][0])[:300]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching with Metadata Filtering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "async_requests = [\n",
    "    cfVect.asimilarity_search(\n",
    "        index_name=vectorize_index_name1,\n",
    "        query=\"Cloudflare services\",\n",
    "        k=2,\n",
    "        md_filter={\"section\": \"Products\"},\n",
    "        return_metadata=\"all\",\n",
    "        # return_values=True\n",
    "    ),\n",
    "    cfVect.asimilarity_search(\n",
    "        index_name=vectorize_index_name2,\n",
    "        query=\"Cloudflare services\",\n",
    "        k=2,\n",
    "        md_filter={\"section\": \"Products\"},\n",
    "        return_metadata=\"all\",\n",
    "        # return_values=True\n",
    "    ),\n",
    "    cfVect.asimilarity_search(\n",
    "        index_name=vectorize_index_name3,\n",
    "        query=\"Cloudflare services\",\n",
    "        k=2,\n",
    "        md_filter={\"section\": \"Products\"},\n",
    "        return_metadata=\"all\",\n",
    "        # return_values=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "async_results = await asyncio.gather(*async_requests);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "[doc.metadata[\"section\"] == \"Products\" for doc in async_results[0]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"{len(async_results[0])} results:\\n{str(async_results[0][-1])[:300]}\")\n",
    "print(f\"{len(async_results[1])} results:\\n{str(async_results[1][0])[:300]}\")\n",
    "print(f\"{len(async_results[1])} results:\\n{str(async_results[2][0])[:300]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Let's finish by deleting all of the indexes we created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "arr_indexes = cfVect.list_indexes()\n",
    "arr_indexes = [x for x in arr_indexes if \"test-langchain\" in x.get(\"name\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "arr_async_requests = [\n",
    "    cfVect.adelete_index(index_name=x.get(\"name\")) for x in arr_indexes\n",
    "]\n",
    "await asyncio.gather(*arr_async_requests);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://developers.cloudflare.com/api/resources/vectorize/\n",
    "\n",
    "https://developers.cloudflare.com/vectorize/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc-langchain-G_cWTCcf-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
